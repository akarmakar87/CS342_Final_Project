{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:07:38.713142Z","iopub.status.busy":"2024-04-20T20:07:38.712641Z","iopub.status.idle":"2024-04-20T20:07:50.003215Z","shell.execute_reply":"2024-04-20T20:07:50.001726Z","shell.execute_reply.started":"2024-04-20T20:07:38.713099Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import datasets, transforms\n","import torch.nn as nn\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import os\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:07:50.007306Z","iopub.status.busy":"2024-04-20T20:07:50.005860Z","iopub.status.idle":"2024-04-20T20:08:47.530029Z","shell.execute_reply":"2024-04-20T20:08:47.528591Z","shell.execute_reply.started":"2024-04-20T20:07:50.007215Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n","genre\n","['Impressionism']                 12847\n","['Realism']                       10534\n","['Romanticism']                    6896\n","['Expressionism']                  6280\n","['Baroque']                        4202\n","['Art Nouveau Modern']             4155\n","['Northern Renaissance']           2550\n","['Naive Art Primitivism']          2299\n","['Rococo']                         2070\n","['Cubism']                         2002\n","['Color Field Painting']           1486\n","['Early Renaissance']              1387\n","['High Renaissance']               1339\n","['Mannerism Late Renaissance']     1275\n","['Minimalism']                     1250\n","['Ukiyo e']                        1159\n","Name: count, dtype: int64\n","1159\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_65/1964804695.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  df = df.groupby('genre', group_keys=False).apply(lambda x: x.sample(min(len(x), min(genre_counts))))\n"]},{"name":"stdout","output_type":"stream","text":["genre\n","2     1159\n","4     1159\n","5     1159\n","6     1159\n","8     1159\n","9     1159\n","11    1159\n","12    1159\n","13    1159\n","15    1159\n","1     1152\n","14    1151\n","0     1149\n","3     1140\n","10    1134\n","7     1120\n","Name: count, dtype: int64\n","1120\n"]}],"source":["parent_dir = \"/kaggle/input/wikiart/\"\n","\n","# Define valid_genres list\n","valid_genres = [\n","    \"Art Nouveau Modern\",\n","    \"Baroque\",\n","    \"Color Field Painting\",\n","    \"Cubism\",\n","    \"Early Renaissance\",\n","    \"Expressionism\",\n","    \"High Renaissance\",\n","    \"Impressionism\",\n","    \"Mannerism Late Renaissance\",\n","    \"Minimalism\",\n","    \"Naive Art Primitivism\",\n","    \"Northern Renaissance\",\n","    \"Realism\",\n","    \"Rococo\",\n","    \"Romanticism\",\n","    \"Ukiyo e\"\n","]\n","\n","# Read the CSV file into a DataFrame\n","df = pd.read_csv(parent_dir + 'classes.csv')\n","\n","# Drop rows with genre_count greater than 1\n","df = df[df['genre_count'] <= 1]\n","\n","# Drop columns except 'filename', 'genre', 'subset'\n","df = df[['filename', 'genre', 'subset']]\n","\n","\n","# Drop rows with genres not in the valid_genres list\n","df = df[df['genre'].apply(lambda x: x[2:-2] in valid_genres)]\n","print(len(df['genre'].unique()))\n","\n","# Reset index after dropping rows\n","df.reset_index(drop=True, inplace=True)\n","\n","genre_counts = df['genre'].value_counts()\n","print(genre_counts)\n","print(min(genre_counts))\n","\n","df = df.groupby('genre', group_keys=False).apply(lambda x: x.sample(min(len(x), min(genre_counts))))\n","\n","# Reset index after dropping rows\n","df.reset_index(drop=True, inplace=True)\n","\n","# Replace genre values with their indices in the valid_genres list\n","df['genre'] = df['genre'].apply(lambda x: valid_genres.index(x[1:-1].split(',')[0].strip()[1:-1]))\n","\n","df = df[df['filename'].apply(lambda x: os.path.exists(parent_dir + x))]\n","\n","genre_counts = df['genre'].value_counts()\n","print(genre_counts)\n","print(min(genre_counts))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:08:47.532088Z","iopub.status.busy":"2024-04-20T20:08:47.531673Z","iopub.status.idle":"2024-04-20T20:08:47.544189Z","shell.execute_reply":"2024-04-20T20:08:47.543209Z","shell.execute_reply.started":"2024-04-20T20:08:47.532052Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                                            filename  genre subset\n","0  Art_Nouveau_Modern/egon-schiele_sunflower-1909...      0  train\n","1      Art_Nouveau_Modern/gustav-klimt_baby-1918.jpg      0  train\n","2  Art_Nouveau_Modern/gustav-klimt_after-the-rain...      0   test\n","3  Art_Nouveau_Modern/ivan-bilibin_bird-of-paradi...      0  train\n","4  Art_Nouveau_Modern/raphael-kirchner_girls-with...      0  train\n"]}],"source":["print(df.head())"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:12:58.580732Z","iopub.status.busy":"2024-04-20T20:12:58.580170Z","iopub.status.idle":"2024-04-20T20:13:10.803585Z","shell.execute_reply":"2024-04-20T20:13:10.802271Z","shell.execute_reply.started":"2024-04-20T20:12:58.580696Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CNNArtModel(\n","  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (maxpool_conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (maxpool_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (avgpool_conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","  (avgpool_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (attention_gate_max): GatedAttention(\n","    (attention_gate): Sequential(\n","      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","      (1): Sigmoid()\n","    )\n","  )\n","  (attention_gate_avg): GatedAttention(\n","    (attention_gate): Sequential(\n","      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n","      (1): Sigmoid()\n","    )\n","  )\n","  (fc1): Linear(in_features=2097152, out_features=512, bias=True)\n","  (fc2): Linear(in_features=512, out_features=16, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n"]}],"source":["class GatedAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(GatedAttention, self).__init__()\n","        self.attention_gate = nn.Sequential(\n","            nn.Conv2d(in_channels, 1, kernel_size=1),  # Reduce channel dimension to 1\n","            nn.Sigmoid()  # Activation to get coefficients between 0 and 1\n","        )\n","    \n","    def forward(self, x):\n","        attention_weights = self.attention_gate(x)\n","        return x * attention_weights  # Element-wise multiplication\n","\n","class CNNArtModel(nn.Module):\n","    def __init__(self, num_classes,input_size):\n","        super(CNNArtModel, self).__init__()\n","        \n","        final_size = (input_size // 4, input_size // 4)  # This accounts for two pooling layers with stride 2 each\n","\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n","        self.maxpool_conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.maxpool_conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        \n","        self.avgpool_conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n","        self.avgpool = nn.AvgPool2d(2)\n","        self.avgpool_conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","\n","        # Attention Gates\n","        self.attention_gate_max = GatedAttention(64)\n","        self.attention_gate_avg = GatedAttention(64)\n","        \n","#       print(\"FC1 SHAPE: \", 64 * 2 * final_size[0] * final_size[1], 512)\n","        \n","        self.fc1 = nn.Linear(2097152, 512)\n","        self.fc2 = nn.Linear(512, num_classes)\n","        self.dropout = nn.Dropout(0.5)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        \n","        maxpool_x = F.relu(self.maxpool_conv2(x))\n","        maxpool_x = self.maxpool(maxpool_x)\n","        maxpool_x = F.relu(self.maxpool_conv3(maxpool_x))\n","        maxpool_x = self.attention_gate_max(maxpool_x)\n","        \n","        avgpool_x = F.relu(self.avgpool_conv2(x))\n","        avgpool_x = self.avgpool(avgpool_x)\n","        avgpool_x = F.relu(self.avgpool_conv3(avgpool_x))\n","        avgpool_x = self.attention_gate_avg(avgpool_x)\n","        \n","        # Flatten the output for each path\n","        maxpool_x = torch.flatten(maxpool_x, 1)\n","        avgpool_x = torch.flatten(avgpool_x, 1)\n","        \n","        # Concatenate the flattened outputs from both paths\n","        combined_x = torch.cat((maxpool_x, avgpool_x), dim=1)\n","        \n","        print(combined_x.shape)\n","        print(self.fc1)\n","        \n","        # Pass through fully connected layers\n","        combined_x = F.relu(self.fc1(combined_x))\n","        combined_x = self.dropout(combined_x)\n","        combined_x = self.fc2(combined_x)\n","        \n","        return combined_x\n","\n","# Example model creation\n","model = CNNArtModel(num_classes=16, input_size=256)\n","print(model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-20T20:13:20.570578Z","iopub.status.busy":"2024-04-20T20:13:20.569127Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([128, 2097152])\n","Linear(in_features=2097152, out_features=512, bias=True)\n","torch.Size([128, 2097152])\n","Linear(in_features=2097152, out_features=512, bias=True)\n"]}],"source":["# Split the dataframe into train and test subsets\n","train_df = df[df['subset'] == 'train']\n","test_df = df[df['subset'] == 'test']\n","\n","\n","class SquarePad:\n","    def __call__(self, image):\n","        w, h = image.size\n","        max_wh = max([w, h])\n","        hp = int((max_wh - w) // 2)\n","        vp = int((max_wh - h) // 2)\n","        padding = (hp, hp, vp, vp)\n","        if hp * 2 + w < 256:\n","            padding = (hp, hp + 1, vp, vp)\n","        if vp * 2 + h < 256:\n","            padding = (hp, hp, vp, vp + 1)\n","            \n","        \n","#         print(\"padding: \", padding)\n","        \n","        image_tensor = torch.tensor(np.array(image), dtype=torch.float).permute(2,0,1)\n","#         print(\"image: \", image_tensor.shape)\n","        \n","        padded_tensor = F.pad(image_tensor,padding, mode='replicate')\n","        \n","#         print(\"padded: \", padded_tensor.shape)\n","        return padded_tensor\n","    \n","\n","def resize_larger_dimension(image, size):\n","    width, height = image.size\n","    aspect_ratio = width / height\n","    if width > height:\n","        new_width = size\n","        new_height = int(size / aspect_ratio)\n","    else:\n","        new_width = int(size * aspect_ratio)\n","        new_height = size\n","#     print(aspect_ratio)\n","#     print(new_width)\n","#     print(new_height)\n","    return image.resize((new_width, new_height))\n","\n","transform = transforms.Compose([\n","    transforms.Lambda(lambda x: resize_larger_dimension(x, 256)),  # Resize the larger dimension to 256 while preserving aspect ratio\n","    SquarePad(),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n","])\n","\n","# Define a custom dataset class\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataframe, transform=None):\n","        self.dataframe = dataframe\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        filename = parent_dir + self.dataframe.iloc[idx]['filename']\n","        image = Image.open(filename).convert('RGB')\n","        label = self.dataframe.iloc[idx]['genre']\n","\n","        if self.transform:\n","            w, h = image.size\n","#             print(\"orig --> w:\", w, \" h:\", h)\n","            image = self.transform(image)\n","#             print(\"new --> w:\", w, \" h:\", h)\n","\n","        return image, label\n","    \n","# Create custom datasets for train and test\n","train_dataset = CustomDataset(train_df, transform=transform)\n","test_dataset = CustomDataset(test_df, transform=transform)\n","\n","# Create train and test dataloaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# write a function that does one training epoch\n","nepoch = 5\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.CrossEntropyLoss()\n","def train_one_epoch(model):\n","    total_loss = 0\n","    count = 0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model.forward(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        count += 1\n","    print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n","    with torch.no_grad():\n","        total_loss = 0\n","        count = 0\n","        for inputs, labels in val_loader:\n","            outputs = model.forward(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            count += 1\n","        print('{:>12s} {:>7.5f}'.format('Val loss:', total_loss/count))\n","\n","    print()\n","\n","# Example of using the train_loader and test_loader in a training loop\n","index = 1\n","train_size = len(train_loader)\n","model = CNNArtModel(16, 256)\n","# for images, labels in train_loader:\n","    \n","for epoch in range(nepoch):\n","    train_one_epoch(model)\n","\n","# for images, labels in test_loader:\n","    \n","#     pass"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2477766,"sourceId":4202543,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
