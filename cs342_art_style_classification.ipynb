{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D3LZ7dNbYKS1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNfsgxB-pbBI",
    "outputId": "1e9534f3-deec-41cb-da95-a43104ffc47c"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/cs342fp/fixed_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eakSV5OZHRbh"
   },
   "outputs": [],
   "source": [
    "valid_genres = [\n",
    "    \"Cubism\", \n",
    "    \"Mannerism_Late_Renaissance\", \n",
    "    \"Color_Field_Painting\", \n",
    "    \"Ukiyo_e\" \n",
    "]\n",
    "\n",
    "# Function to sample files from each child directory\n",
    "def sample_files(parent_dir, num_samples):\n",
    "    data = []\n",
    "    # Iterate through each folder in the parent directory\n",
    "    for folder in os.listdir(parent_dir):\n",
    "\n",
    "        if folder not in valid_genres:\n",
    "          continue\n",
    "\n",
    "        folder_path = os.path.join(parent_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "          continue\n",
    "\n",
    "        files_in_folder = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "        # If there are fewer than num_samples files, sample all of them\n",
    "        sampled_files = random.sample(files_in_folder, min(num_samples, len(files_in_folder)))\n",
    "\n",
    "        # Get the genre (name of the child folder)\n",
    "        genre = folder\n",
    "\n",
    "        # Append the filename and genre to the data list\n",
    "        for filename in sampled_files:\n",
    "            data.append({'filename': os.path.join(genre, filename), 'genre': genre})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Set the parent directory\n",
    "parent_directory = \"/content/drive/MyDrive/cs342fp/fixed_data/kaggle/working/\"\n",
    "\n",
    "# Sample files and create a dataframe\n",
    "sampled_data = sample_files(parent_directory, num_samples=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyp62zC5LQ4U",
    "outputId": "03615635-18ce-45bf-cc93-5d655905d2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               filename  genre subset\n",
      "0     Color_Field_Painting/mark-rothko_untitled-blue...      2  train\n",
      "1     Ukiyo_e/katsushika-hokusai_peasants-in-autumn.jpg      3  train\n",
      "2               Ukiyo_e/ito-jakuchu_rousho-hakkeizu.jpg      3  train\n",
      "3     Color_Field_Painting/morris-louis_number-32-19...      2  train\n",
      "4     Mannerism_Late_Renaissance/tintoretto_portrait...      1  train\n",
      "...                                                 ...    ...    ...\n",
      "4395                Ukiyo_e/hiroshige_four-swallows.jpg      3  train\n",
      "4396  Color_Field_Painting/barnett-newman_eleventh-s...      2  train\n",
      "4397     Cubism/pablo-picasso_glass-and-fruits-1908.jpg      0  train\n",
      "4398  Ukiyo_e/utagawa-kunisada_kagamiiwa-hamanosuke-...      3   test\n",
      "4399  Color_Field_Painting/gene-davis_split-beat-196...      2  train\n",
      "\n",
      "[4400 rows x 3 columns]\n",
      "subset\n",
      "train    3520\n",
      "test      880\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(sampled_data)\n",
    "\n",
    "# Shuffle the rows of the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create a column called \"subset\" and randomly set values\n",
    "df['subset'] = 'train'  # Set default value to 'train'\n",
    "test_indices = random.sample(range(len(df)), int(0.2 * len(df)))  # Select 20% of rows for testing\n",
    "df.loc[test_indices, 'subset'] = 'test'  # Set subset value to 'test' for selected rows\n",
    "\n",
    "df['genre'] = df['genre'].apply(lambda x: valid_genres.index(x))\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)\n",
    "print(df['subset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FMItCzZbMhkX"
   },
   "outputs": [],
   "source": [
    "parent_dir = \"kaggle/working/\"\n",
    "img_max = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfZHkyORYKS3",
    "outputId": "c22a458c-4b42-4909-aa21-c7a049c8d31f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNArtModel(\n",
      "  (path1_conv1): Conv2d(3, 16, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
      "  (path1_pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (path1_conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (path1_pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (path1_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (path1_pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (path2_conv1): Conv2d(3, 16, kernel_size=(6, 6), stride=(1, 1), padding=(1, 1))\n",
      "  (path2_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (path2_conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (path2_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (path2_conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (path2_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gated_attention): SimpleGatedAttention(\n",
      "    (attention_weights): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleGatedAttention(nn.Module):\n",
    "    def __init__(self, channel_size):\n",
    "        super(SimpleGatedAttention, self).__init__()\n",
    "        self.attention_weights = nn.Sequential(\n",
    "            nn.Conv2d(channel_size * 2, channel_size, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        g = self.attention_weights(torch.cat((x1, x2), dim=1))\n",
    "        return x1 * g + x2 * (1 - g)\n",
    "\n",
    "class CNNArtModel(nn.Module):\n",
    "    def __init__(self, num_classes=16):\n",
    "        super(CNNArtModel, self).__init__()\n",
    "        # Path 1 for color usage\n",
    "        self.path1_conv1 = nn.Conv2d(3, 16, kernel_size=6, padding=1)\n",
    "        self.path1_pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.path1_conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.path1_pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.path1_conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.path1_pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Path 2 for textural qualities\n",
    "        self.path2_conv1 = nn.Conv2d(3, 16, kernel_size=6, padding=1)\n",
    "        self.path2_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.path2_conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.path2_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.path2_conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.path2_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Gated attention to merge paths\n",
    "        self.gated_attention = SimpleGatedAttention(64)\n",
    "\n",
    "        # Global average pooling and final classifier\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Path 1\n",
    "        x1 = F.relu(self.path1_conv1(x))\n",
    "        x1 = self.path1_pool1(x1)\n",
    "        x1 = F.relu(self.path1_conv2(x1))\n",
    "        x1 = self.path1_pool2(x1)\n",
    "        x1 = F.relu(self.path1_conv3(x1))\n",
    "        x1 = self.path1_pool3(x1)\n",
    "\n",
    "        # Path 2\n",
    "        x2 = F.relu(self.path2_conv1(x))\n",
    "        x2 = self.path2_pool1(x2)\n",
    "        x2 = F.relu(self.path2_conv2(x2))\n",
    "        x2 = self.path2_pool2(x2)\n",
    "        x2 = F.relu(self.path2_conv3(x2))\n",
    "        x2 = self.path2_pool3(x2)\n",
    "\n",
    "        # Merge paths with gated attention\n",
    "        x = self.gated_attention(x1, x2)\n",
    "\n",
    "        # Global average pooling and classification\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "print(CNNArtModel(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_6m_C6L2YKS3"
   },
   "outputs": [],
   "source": [
    "# Split the dataframe into train and test subsets\n",
    "train_df = df[df['subset'] == 'train']\n",
    "test_df = df[df['subset'] == 'test']\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "\n",
    "        padded_tensor = torch.tensor(np.array(image), dtype=torch.float).permute(2,0,1)\n",
    "\n",
    "        while padded_tensor.shape != (3,img_max,img_max):\n",
    "            h = padded_tensor.shape[1]\n",
    "            w = padded_tensor.shape[2]\n",
    "\n",
    "            max_wh = max([w, h])\n",
    "            hp = int((max_wh - w) // 2)\n",
    "            vp = int((max_wh - h) // 2)\n",
    "            padding = (hp, hp, vp, vp)\n",
    "            if hp * 2 + w < img_max:\n",
    "                padding = (hp, hp + 1, vp, vp)\n",
    "            if vp * 2 + h < img_max:\n",
    "                padding = (hp, hp, vp, vp + 1)\n",
    "\n",
    "            padding = (min(padding[0], w-1),min(padding[1], w-1),min(padding[2], h-1),min(padding[3], h-1))\n",
    "\n",
    "            padded_tensor = F.pad(padded_tensor,padding, mode='reflect')\n",
    "\n",
    "        assert(padded_tensor.shape == (3,img_max,img_max))\n",
    "        return padded_tensor\n",
    "\n",
    "def resize_larger_dimension(image, size):\n",
    "    width, height = image.size\n",
    "    aspect_ratio = width / height\n",
    "    if width > height:\n",
    "        new_width = size\n",
    "        new_height = int(size / aspect_ratio)\n",
    "    else:\n",
    "        new_width = int(size * aspect_ratio)\n",
    "        new_height = size\n",
    "\n",
    "    return image.resize((new_width, new_height))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: resize_larger_dimension(x, img_max)),  # Resize the larger dimension to img_max while preserving aspect ratio\n",
    "    SquarePad(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.join(parent_dir, self.dataframe.iloc[idx]['filename'])\n",
    "        image = Image.open(filename).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx]['genre']\n",
    "\n",
    "        if self.transform:\n",
    "            w, h = image.size\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create custom datasets for train and test\n",
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)\n",
    "\n",
    "# Create train and test dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3vJq2wVYKS4",
    "outputId": "4a7724da-23c6-4830-d6bc-944dcf6416ca"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNArtModel(num_classes=4).to(device)\n",
    "\n",
    "# Modify the train_one_epoch function to move inputs and labels to GPU\n",
    "def train_one_epoch(model, train_loader, test_loader, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        print(f'Train loss after iteration {count}: {total_loss/count}')\n",
    "    print('{:>12s} {:>7.5f}'.format('Train loss:', total_loss/count))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    true, pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels  in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            predicted = torch.argmax(outputs, dim=1) # get predicted class label for each test example.\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            true.append(labels)\n",
    "            pred.append(predicted)\n",
    "    acc = (100 * correct / total)\n",
    "    print('accuracy: %0.3f' % (acc))\n",
    "    print()\n",
    "    return acc\n",
    "\n",
    "# Example of using the train_loader and val_loader in a training loop\n",
    "nepoch = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    acc = train_one_epoch(model, train_loader, test_loader, optimizer, criterion)\n",
    "    scheduler.step()\n",
    "    torch.save(model, f'art_model_{epoch}_{round(acc,1)}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3vyIXAtslDJ"
   },
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# 3. Run inference and collect predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.numpy())\n",
    "        true_labels.extend(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "jlYYquW1Cs5A",
    "outputId": "4363b2ba-3047-406b-e97c-cb2563c955a1"
   },
   "outputs": [],
   "source": [
    "# Assuming valid_genres contains the list of genre strings\n",
    "valid_genres = np.array(valid_genres)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(valid_genres[true_labels], valid_genres[predictions], labels=valid_genres)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=valid_genres, yticklabels=valid_genres)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title(f'Confusion Matrix\\nAccuracy: {accuracy:.4f}')\n",
    "plt.show()\n",
    "\n",
    "# Print accuracy\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2477766,
     "sourceId": 4202543,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
